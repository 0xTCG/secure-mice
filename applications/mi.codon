""" Multiple imputation experiments from https://www.nature.com/articles/s41467-020-19270-2 """

import math, time

from tqdm import tqdm

from numpy.ndarray import ndarray
from numpy.create import array, zeros, ones, ones_like

from sequre.stdlib.learn.log_reg import LogReg
from sequre.stdlib.learn.lin_reg import LinReg
from sequre.stdlib.learn.mi import MI, MICE
from sequre.types.sharetensor import Sharetensor
from sequre.types.multiparty_union import MPU
from sequre.utils.io import read_matrix, log


# Temp solution until Union types are fixed.
class RegUnion[T]:
    _lin: LinReg[T]
    _log: LogReg[T]

    def __init__(self, mpc, lin: LinReg[T]):
        self._lin = lin
        self._log = LogReg[T](mpc)
    
    def __init__(self, mpc, log: LogReg[T]):
        self._lin = LinReg[T](mpc)
        self._log = log
    
    def is_lin(self) -> bool:
        return bool(self._lin.optimizer)
    
    def is_log(self) -> bool:
        return bool(self._log.optimizer)
    
    def fit(self, *args, **kwargs) -> RegUnion[T]:
        if self.is_lin():
            self._lin = self._lin.fit(*args, **kwargs)
        elif self.is_log():
            self._log = self._log.fit(*args, **kwargs)
        return self
    
    def predict(self, *args, **kwargs):
        if self.is_lin():
            return self._lin.predict(*args, **kwargs)

        return self._log.predict(*args, **kwargs)
    
    def loss[T](self, mpc, X: T, y: T) -> T:
        if self.is_lin():
            return self._lin.loss(mpc, X, y)

        return self._log.loss(mpc, X, y)
    
    def randomize_weights(self, *args, **kwargs):
        if self.is_lin():
            self._lin.randomize_weights(*args, **kwargs)
        else:
            self._log.randomize_weights(*args, **kwargs)
    

def _match_model[T](mpc, scenario: str, secure_im_initial_w: T, im_optimizer: str) -> RegUnion[T]:
    if scenario == "paper_scenario_2" or scenario == "log" or scenario == "midn_scenario_2":
        return RegUnion(mpc, log=LogReg(secure_im_initial_w.copy(), im_optimizer))
    elif scenario == "paper_scenario_1" or scenario == "lin" or scenario == "midn_scenario_1":
        return RegUnion(mpc, lin=LinReg(secure_im_initial_w.copy(), im_optimizer))
    else:
        raise ValueError(f"Invalid scenario: {scenario}")


def load_scenario_mi_data(scenario: str, rows_count: int, cols_count: int):
    path = f"data/mi/{scenario}/{rows_count}"
    with open(f"{path}/tampered_data.txt") as f:
        incomplete_data = array(read_matrix(f, rows_count, cols_count, binary=False, TP=float))
    with open(f"{path}/non_tampered_data.txt") as f:
        complete_data = array(read_matrix(f, rows_count, cols_count, binary=False, TP=float))
    with open(f"{path}/non_tampered_labels.txt") as f:
        labels = array(read_matrix(f, rows_count, 1, binary=False, TP=float))
    with open(f"{path}/miss_rows.txt") as f:
        miss_rows = read_matrix(f, rows_count, 1, binary=False, TP=int).flatten()
    
    feature_count = cols_count
    im_initial_w = ones((feature_count, 1))
    fm_initial_w = ndarray.rand((feature_count + 1, 1), "uniform")
    im_optimizer = "bgd"
    fm_optimizer = "bgd"

    im_step_size = 1 / (1 << 11)
    fm_step_size = 1 / (1 << 12)
    if "midn_scenario_2" in scenario:
        im_step_size = 1 / (1 << 7)
        im_step_size /= (1 << (rows_count // 2000))
    
    return (incomplete_data, complete_data, labels, miss_rows,
        im_initial_w, fm_initial_w, im_step_size, fm_step_size,
        im_optimizer, fm_optimizer)


def load_real_mi_data(rows_count: int = 86322, complete_data_size: int = 14940, feature_count: int = 15):
    with open("data/mi/real/incomplete_data.txt") as f:
        incomplete_data = array(read_matrix(f, rows_count, feature_count, binary=False, TP=float))
    with open("data/mi/real/complete_data.txt") as f:
        complete_data = array(read_matrix(f, complete_data_size, feature_count, binary=False, TP=float))
    with open("data/mi/real/labels.txt") as f:
        labels = array(read_matrix(f, rows_count, 1, binary=False, TP=float))
    with open("data/mi/real/miss_rows.txt") as f:
        miss_rows = read_matrix(f, feature_count, rows_count, binary=False, TP=int)
    with open("data/mi/real/col_types.txt") as f:
        col_types = [line.strip() for line in f.readlines()]
    
    im_step_size = 1 / (1 << 17)
    fm_step_size = 1 / (1 << 19)
    im_initial_w = ones((feature_count, 1)) * (1 / (1 << 10))
    fm_initial_w = ones((feature_count + 1, 1)) * (1 / (1 << 10))
    im_optimizer = "bgd"
    fm_optimizer = "bgd"

    return (incomplete_data, complete_data, labels, miss_rows, col_types,
        im_initial_w, fm_initial_w, im_step_size, fm_step_size,
        im_optimizer, fm_optimizer)


def evaluate_scenario(
        mpc, backend: Static[str], msg: Static[str], scenario, im_epochs: int, fm_epochs: int, noise_scale: float,
        mi_factor: int, miss_col: int, dump_data: bool, verbose: bool, modulus):
    @python
    def py_roc_auc(y_true: list[float], y_score: list[float]) -> float:
        from sklearn.metrics import roc_auc_score
        return roc_auc_score(y_true, y_score)
    
    (data, complete_data, labels, miss_rows,
     im_initial_w, fm_initial_w, im_step_size, fm_step_size,
     im_optimizer, fm_optimizer) = scenario
    
    if backend == "sequre":
        secure_complete_data = Sharetensor.enc(mpc, complete_data, 0, modulus)
        secure_data = Sharetensor.enc(mpc, data, 0, modulus)
        secure_labels = Sharetensor.enc(mpc, labels, 0, modulus)
        secure_im_initial_w = Sharetensor.enc(mpc, im_initial_w, 0, modulus)
        secure_fm_initial_w = Sharetensor.enc(mpc, fm_initial_w, 0, modulus)
    elif backend == "shechi":
        secure_complete_data = MPU.partition(mpc, complete_data)
        secure_data = MPU.partition(mpc, data)
        secure_labels = MPU.partition(mpc, labels)
        secure_im_initial_w = MPU.additive(mpc, im_initial_w)
        secure_fm_initial_w = MPU.additive(mpc, fm_initial_w)
    else:
        compile_error("Invalid backend provided")

    secure_impute_model = _match_model(mpc, msg, secure_im_initial_w, im_optimizer)
    secure_fit_model = LinReg(secure_fm_initial_w, fm_optimizer)

    secure_mi, secure_imputed_datasets = MI(mi_factor, secure_impute_model, secure_fit_model).fit(
        mpc, secure_data, secure_labels, miss_rows, miss_col,
        im_step_size, fm_step_size, im_epochs, fm_epochs, noise_scale, "batched", verbose=verbose)

    if dump_data:
        for i, data in enumerate(secure_imputed_datasets):
            revealed_data = data.reveal(mpc).tolist()
            if mpc.pid == 2:
                log("", revealed_data,
                    path=f'data/mi/dump/sequre_imputed_dataset_{msg}_{i + 1}.txt',
                    mode='w', separator=' ')

    predicted_data = secure_mi.model.predict(mpc, secure_complete_data).reveal(mpc)
    
    abs_diff = (predicted_data - labels).abs()
    assert abs_diff.shape == predicted_data.shape
    
    y_hat_y_mean = abs_diff.mean()
    y_hat_y_sd = ndarray.std(abs_diff)

    imputed_cols, imputed_cols_prob = [], []
    for imputed_dataset in secure_imputed_datasets:
        raw_data = imputed_dataset.reveal(mpc)
        imputed_col = array([raw_data[miss_row, miss_col] for miss_row in miss_rows])
        imputed_cols_prob.append(imputed_col)
        if "_2" in msg:
            imputed_col = imputed_cols_prob[-1].round()
        imputed_cols.append(imputed_col)
    
    expected_imputation = array([complete_data[miss_row, miss_col] for miss_row in miss_rows])
    imputation_abs_diff = ([(imputed_col - expected_imputation).abs() for imputed_col in imputed_cols]).sum() / len(imputed_cols)
    imputation_diff_mean = imputation_abs_diff.mean()
    imputation_diff_std = imputation_abs_diff.std()
    imputation_accuracy = ([((imputed_col == expected_imputation).astype(int).sum() / len(expected_imputation)) for imputed_col in imputed_cols]).mean()
    imputation_auc = ([py_roc_auc(expected_imputation.tolist(), imputed_col.tolist()) for imputed_col in imputed_cols_prob]).mean()

    return secure_mi.model.coef_.reveal(mpc), y_hat_y_mean, y_hat_y_sd, imputation_diff_mean, imputation_diff_std, imputation_accuracy, imputation_auc


def benchmark_scenario(mpc, backend: Static[str], msg: Static[str], runs_count: int, scenario, im_epochs: int,
                       fm_epochs: int, noise_scale: float, mi_factor: int, miss_col: int, dump_data: bool, verbose: bool, modulus):
    cols_count = scenario[0].shape[1]
    coefs = zeros((runs_count, cols_count))
    y_hat_y_means = zeros((runs_count,))
    y_hat_y_sds = zeros((runs_count,))
    imp_means = zeros((runs_count,))
    imp_sds = zeros((runs_count,))
    imp_accs = zeros((runs_count,))
    imp_aucs = zeros((runs_count,))
    runtimes = zeros((runs_count,))
    network_bws = zeros((runs_count,))
    
    for i in tqdm(mpc, range(runs_count), f"Evaluating {msg}"):
        time_s, net_s = time.time(), mpc.stats.bytes_sent
        coef, y_hat_y_mean, y_hat_y_sd, imp_mean, imp_sd, imp_acc, imp_auc = evaluate_scenario(
            mpc, backend, msg, scenario, im_epochs, fm_epochs, noise_scale,
            mi_factor, miss_col, dump_data, verbose, modulus)
        time_e, net_e = time.time(), mpc.stats.bytes_sent

        coefs[i] = coef
        y_hat_y_means[i] = y_hat_y_mean
        y_hat_y_sds[i] = y_hat_y_sd
        imp_means[i] = imp_mean
        imp_sds[i] = imp_sd
        imp_accs[i] = imp_acc
        imp_aucs[i] = imp_auc
        runtimes[i] = time_e - time_s
        network_bws[i] = net_e - net_s
    
    return coefs, y_hat_y_means, y_hat_y_sds, imp_means, imp_sds, imp_accs, imp_aucs, runtimes, network_bws


def evaluate_real_setup(mpc, backend: Static[str], real_data, mi_factor: int, im_epochs: int, fm_epochs: int, noise_scale: float, dump_data: bool, verbose: bool, modulus):
    (incomplete_data, complete_data, labels,
     miss_row_masks, col_types,
     im_initial_w, fm_initial_w, im_step_size, fm_step_size,
     im_optimizer, fm_optimizer) = real_data

    if backend == "sequre":
        secure_incomplete_data = Sharetensor.enc(mpc, incomplete_data, 0, modulus)
        secure_complete_data = Sharetensor.enc(mpc, complete_data, 0, modulus)
        secure_labels = Sharetensor.enc(mpc, labels, 0, modulus)
        secure_im_initial_w = Sharetensor.enc(mpc, im_initial_w, 0, modulus)
        secure_fm_initial_w = Sharetensor.enc(mpc, fm_initial_w, 0, modulus)
    elif backend == "shechi":
        secure_incomplete_data = MPU.partition(mpc, incomplete_data)
        secure_complete_data = MPU.partition(mpc, complete_data)
        secure_labels = MPU.partition(mpc, labels)
        secure_im_initial_w = MPU.additive(mpc, im_initial_w)
        secure_fm_initial_w = MPU.additive(mpc, fm_initial_w)
    else:
        compile_error("Invalid backend provided")
    
    secure_impute_models = [_match_model(mpc, col_type, secure_im_initial_w, im_optimizer) for col_type in col_types]
    secure_fit_model = LinReg(secure_fm_initial_w, fm_optimizer)
    
    secure_mi, secure_imputed_datasets = MICE(mi_factor, secure_impute_models, secure_fit_model).fit(
        mpc, secure_incomplete_data, secure_complete_data, secure_labels, miss_row_masks,
        im_step_size, fm_step_size, im_epochs, fm_epochs, noise_scale, verbose=verbose)
    
    if dump_data:
        for i, data in enumerate(secure_imputed_datasets):
            revealed_data = data.reveal(mpc).tolist()
            if mpc.pid == 2:
                log("", revealed_data, path=f'data/mi/dump/sequre_imputed_dataset_real_{i + 1}.txt', mode='w', separator=' ')
    
    predicted_data = secure_mi.model.predict(mpc, secure_incomplete_data).reveal(mpc)
    abs_diff = (predicted_data - labels).abs()
    assert abs_diff.shape == predicted_data.shape
    
    y_hat_y_mean = abs_diff.mean()
    y_hat_y_sd = ndarray.std(abs_diff)
    
    return y_hat_y_mean, y_hat_y_sd


def benchmark_real_setup(mpc, backend: Static[str], runs_count: int, real_data, mi_factor: int,
                         im_epochs: int, fm_epochs: int, noise_scale: float,
                         dump_data: bool, verbose: bool, modulus):
    y_hat_y_means = zeros((runs_count,))
    y_hat_y_sds = zeros((runs_count,))
    runtimes = zeros((runs_count,))
    network_bws = zeros((runs_count,))
    
    for i in tqdm(mpc, range(runs_count), "Evaluating real setup"):
        time_s, net_s = time.time(), mpc.stats.bytes_sent
        y_hat_y_mean, y_hat_y_sd = evaluate_real_setup(mpc, backend, real_data, mi_factor, im_epochs, fm_epochs, noise_scale, dump_data, verbose, modulus)
        time_e, net_e = time.time(), mpc.stats.bytes_sent
        
        y_hat_y_means[i] = y_hat_y_mean
        y_hat_y_sds[i] = y_hat_y_sd
        runtimes[i] = time_e - time_s
        network_bws[i] = net_e - net_s
    
    return zeros((runs_count, 15)), y_hat_y_means, y_hat_y_sds, zeros((runs_count,)) - 1, zeros((runs_count,)) - 1, zeros((runs_count,)) - 1, zeros((runs_count,)) - 1, runtimes, network_bws


def print_stats(mpc, msg: Static[str], coefs, biases, sds, imp_biases, imp_sds, imp_accs, imp_aucs, runtimes, network_bws):
    csv_out_path = "temp_mi_stats_sequre.csv"
    
    with open(csv_out_path, "a+") as f:
        if "scenario" in msg.lower():
            coefs_truth = ones_like(coefs[0])
            coefs_bias = math.sqrt((ndarray.mean(coefs, axis=0) - coefs_truth).flatten().dot(axis=0))
            coefs_sd = math.sqrt(ndarray.mean((coefs - ndarray.mean(coefs, axis=0)).dot(axis=1)))
            coefs_rmse = math.sqrt(ndarray.mean((coefs - coefs_truth).dot(axis=1).flatten()))

            print(f"\tCP{mpc.pid}: {msg} | Sequre MI coefs bias: {coefs_bias}")
            print(f"\tCP{mpc.pid}: {msg} | Sequre MI coefs SD: {coefs_sd}")
            print(f"\tCP{mpc.pid}: {msg} | Sequre MI coefs rMSE: {coefs_rmse}")

        if mpc.pid == 1:
            f.write(f"{coefs_bias},{coefs_sd},{coefs_rmse}," if "scenario" in msg.lower() else ",,,")

        print(f"\tCP{mpc.pid}: {msg} | Sequre MI runtime (s) | mean: {runtimes.mean()}, min: {runtimes.min()}, max: {runtimes.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI network bandwidth (bytes) | mean: {network_bws.mean()}, min: {network_bws.min()}, max: {network_bws.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI y_hat - y mean | mean: {biases.mean()}, min: {biases.min()}, max: {biases.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI y_hat - y SD | mean: {sds.mean()}, min: {sds.min()}, max: {sds.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI imputation diff mean | mean: {imp_biases.mean()}, min: {imp_biases.min()}, max: {imp_biases.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI imputation diff SD | mean: {imp_sds.mean()}, min: {imp_sds.min()}, max: {imp_sds.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI imputation accuracy | mean: {imp_accs.mean()}, min: {imp_accs.min()}, max: {imp_accs.max()}")
        print(f"\tCP{mpc.pid}: {msg} | Sequre MI imputation AUC | mean: {imp_aucs.mean()}, min: {imp_aucs.min()}, max: {imp_aucs.max()}")

        if mpc.pid == 1:
            f.write(f"{biases.mean()},{sds.mean()},{imp_biases.mean()},{imp_sds.mean()},{imp_accs.mean()},{runtimes.mean()},{network_bws.mean()}\n")


def e2e_scenario_run(mpc, msg: Static[str], backend: Static[str], rows, cols, runs_count,
            im_epochs, fm_epochs, noise_scale, mi_factor,
            miss_col, dump_data, verbose, modulus):
    data = load_scenario_mi_data(msg, rows, cols)

    stats = benchmark_scenario(
        mpc, backend, msg, runs_count, data,
        im_epochs, fm_epochs, noise_scale, mi_factor,
        miss_col, dump_data, verbose, modulus)
    
    print_stats(mpc, msg, *stats)


def benchmark_mi(mpc, backend: Static[str], mi_factor: int, im_epochs: int, fm_epochs: int, noise_scale: float,
                 dump_data: bool, scenarios_runs_count: int, midn_scenarios_rows: int, midn_scenarios_cols: int,
                 paper_scenarios_rows: int, paper_scenarios_cols: int, scenarios_miss_col: int, real_setup_runs_count: int,
                 verbose: bool, run_scenarios: bool, run_real_setup: bool, modulus):
    if run_scenarios:
        midn_scenario_params = (
            midn_scenarios_rows, midn_scenarios_cols, scenarios_runs_count,
            im_epochs, fm_epochs, noise_scale, mi_factor, scenarios_miss_col, dump_data, verbose, modulus)

        paper_scenario_params = (
            paper_scenarios_rows, paper_scenarios_cols, scenarios_runs_count,
            im_epochs, fm_epochs, noise_scale, mi_factor, scenarios_miss_col, dump_data, verbose, modulus)

        e2e_scenario_run(mpc, "midn_scenario_1", backend, *midn_scenario_params)
        e2e_scenario_run(mpc, "midn_scenario_2", backend, *midn_scenario_params)
    
        e2e_scenario_run(mpc, "paper_scenario_1", backend, *paper_scenario_params)
        e2e_scenario_run(mpc, "paper_scenario_2", backend, *paper_scenario_params)
    
    if run_real_setup:  # Real setup not implemented in Shechi
        # Real data
        real = load_real_mi_data()
        
        real_stats = benchmark_real_setup(
            mpc, backend, real_setup_runs_count, real, mi_factor,
            im_epochs, fm_epochs, noise_scale, dump_data, verbose, modulus)

        print_stats(mpc, "Real data", *real_stats)
